{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import ollama\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de la base Chroma existante...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\cyrie\\AppData\\Local\\Temp\\ipykernel_26440\\3523803818.py:36: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Chroma prÃªte Ã  Ãªtre utilisÃ©e\n"
     ]
    }
   ],
   "source": [
    "from src.config import PATH_PROJECT, PROMPT_SYSTEM\n",
    "\n",
    "# PATH_PROJECT = r'C:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project'\n",
    "PATH_PROJECT = r'C:\\Users\\cyrie\\Documents\\Cours\\2024-2025_3A\\ENS_MVA\\LLM\\mva-llm-project'\n",
    "CHROMA_PERSIST_DIR = os.path.join(PATH_PROJECT, \"data/chroma_db/\")\n",
    "\n",
    "if not os.path.exists(CHROMA_PERSIST_DIR) or not os.listdir(CHROMA_PERSIST_DIR):\n",
    "    print(\"ðŸ†• CrÃ©ation de la base Chroma et indexation des documents...\")\n",
    "\n",
    "    # Charger les fichiers PDF\n",
    "    data_dir = os.path.join(PATH_PROJECT, \"data/\")\n",
    "    documents = []\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(data_dir, file)\n",
    "            pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "            documents.extend(pdf_loader.load())\n",
    "\n",
    "    # Diviser les documents en chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': device}, encode_kwargs={'device': device})\n",
    "\n",
    "    # Initialiser Chroma avec les embeddings \n",
    "    vectorstore = Chroma.from_documents(docs, embedding_model, persist_directory=CHROMA_PERSIST_DIR)\n",
    "    vectorstore.persist()\n",
    "    print(\"Base Chroma sauvegardÃ©e avec succÃ¨s\")\n",
    "\n",
    "else:\n",
    "    print(\"Chargement de la base Chroma existante...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': device}, encode_kwargs={'device': device})\n",
    "    vectorstore = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model)\n",
    "\n",
    "print(\"Base Chroma prÃªte Ã  Ãªtre utilisÃ©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "Mood disorders\n",
      "â€¢\t Depressive symptoms include depressed mood or markedly diminished interest or \n",
      "pleasure in activities.\n",
      "â€¢\t There are multiple anxiety symptoms, which may include feeling nervous, anxious or on \n",
      "edge, not being able to control worrying thoughts, fear that something awful will happen, \n",
      "having trouble relaxing, muscle tension or sympathetic autonomic symptoms.\n",
      "â€¢\t Neither the depressive nor the anxiety symptoms â€“ considered separately â€“ are sufficiently\n",
      "guilt, anger, denial, blame, difficulty accepting the death, feeling one has lost a part of oneâ€™s self, \n",
      "inability to experience positive mood, emotional numbness, difficulty in engaging with social or \n",
      "other activities). Some common symptoms of prolonged grief disorder are similar to those observed \n",
      "in a depressive episode (e.g. sadness, loss of interest in activities, social withdrawal, feelings of guilt,\n",
      "â€¢\t The symptoms include:\n",
      "â€¢\t at least one affective symptom such as mood lability, irritability, depressed mood or \n",
      "anxiety;\n",
      "â€¢\t additional somatic or cognitive symptoms such as lethargy, joint pain, overeating, \n",
      "hypersomnia, breast tenderness, swelling of extremities, concentration difficulties or \n",
      "forgetfulness.\n",
      "â€¢\t The symptoms are not better accounted for by another mental disorder (e.g. a mood \n",
      "disorder, an anxiety or fear-related disorder).\n",
      "6A7Z\n",
      "GA34.41\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the symptoms of depression ?\"\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)  # Afficher les documents les plus pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "# prompt = f\"Contexte:\\n{context}\\n\\nQuestion: {query}\\nRÃ©ponse:\"\n",
    "\n",
    "\n",
    "# response = ollama.chat(model=\"deepseek-coder-v2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "# print(response['message'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_excel(os.path.join(PATH_PROJECT, \"data/\") + \"dataset.xlsx\").fillna(\"Non renseignÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrie\\Documents\\Cours\\2024-2025_3A\\ENS_MVA\\LLM\\mva-llm-project\\notebooks\\..\\src\\inference.py:222: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  â€¢ Relevant condition names for History relation: [Condition names]'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_code_with_rag\n",
    "from src.config import PROMPT_SYSTEM_pandas, PROMPT_SYSTEM_EXPERT_pandas\n",
    "code = get_code_with_rag(model = \"mistral\", model_expert = \"mistral\",\n",
    "                  w_model_expert = True,\n",
    "                  prompt_system=PROMPT_SYSTEM_pandas, \n",
    "                  prompt_system_expert=PROMPT_SYSTEM_EXPERT_pandas, \n",
    "                  lan_logic = 'pandas', vectorstore=vectorstore) \n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama-3.2\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[43mget_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\notebooks\\..\\src\\inference.py:73\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(model)\u001b[0m\n",
      "\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_code\u001b[39m(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepseek-coder-v2\u001b[39m\u001b[38;5;124m'\u001b[39m) : \n",
      "\u001b[0;32m     66\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mNow, translate the following criteria into python code WITH THE PACKAGE pyDatalog for Bipolar I, Bipolar II, Single Episode Depressive Disorder, and\u001b[39m\n",
      "\u001b[0;32m     67\u001b[0m \u001b[38;5;124mRecurrent Depressive Disorder. IT MUST BE IN PYTHON WITH THE PACKAGE pyDatalog.\u001b[39m\n",
      "\u001b[0;32m     68\u001b[0m \u001b[38;5;124mâ€¢ Mood Episode criterion: [Depressive, Manic, Mixed, and Hypomanic Episode criteria from ICD-11 CDDR].\u001b[39m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     71\u001b[0m \u001b[38;5;124mâ€¢ Relevant symptom names for Observed relation: [Symptom names]\u001b[39m\n",
      "\u001b[0;32m     72\u001b[0m \u001b[38;5;124mâ€¢ Relevant condition names for History relation: [Condition names]\u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[1;32m---> 73\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n",
      "\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT_SYSTEM\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     79\u001b[0m     code \u001b[38;5;241m=\u001b[39m traitement_reponse(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;32m     80\u001b[0m     check_expert(model, code)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n",
      "\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n",
      "\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n",
      "\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n",
      "\u001b[0;32m    302\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n",
      "\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n",
      "\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n",
      "\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n",
      "\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"llama-3.2\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "from src.inference import get_code_with_rag\n",
    "from src.config import PROMPT_SYSTEM_pandas, PROMPT_SYSTEM_EXPERT_pandas\n",
    "get_code_with_rag(model = \"mistral\", model_expert = \"deepseek-coder-v2\",\n",
    "                  w_model_expert = True,\n",
    "                  prompt_system=PROMPT_SYSTEM_pandas, \n",
    "                  prompt_system_expert=PROMPT_SYSTEM_EXPERT_pandas, \n",
    "                  lan_logic = 'pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import get_code_with_rag\n",
    "from src.config import PROMPT_SYSTEM_pandas, PROMPT_SYSTEM_EXPERT_pandas\n",
    "get_code_with_rag(model = \"deepseek-coder-v2\", model_expert = \"deepseek-coder-v2\",\n",
    "                  w_model_expert = True,\n",
    "                  prompt_system=PROMPT_SYSTEM_pandas, \n",
    "                  prompt_system_expert=PROMPT_SYSTEM_EXPERT_pandas, \n",
    "                  lan_logic = 'pandas') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kanren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de la base Chroma existante...\n",
      "Base Chroma prÃªte Ã  Ãªtre utilisÃ©e\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_code_with_rag\n",
    "from src.config import PROMPT_SYSTEM_kanren ,PROMPT_SYSTEM_EXPERT_kanren \n",
    "get_code_with_rag(model = \"llama3.2\", model_expert = \"llama3.2\",w_model_expert = True,prompt_system=PROMPT_SYSTEM_kanren, prompt_system_expert=PROMPT_SYSTEM_EXPERT_kanren , lan_logic = 'karen') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama-3.2\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\notebooks\\..\\src\\inference.py:73\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_code\u001b[39m(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepseek-coder-v2\u001b[39m\u001b[38;5;124m'\u001b[39m) : \n\u001b[0;32m     66\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mNow, translate the following criteria into python code WITH THE PACKAGE pyDatalog for Bipolar I, Bipolar II, Single Episode Depressive Disorder, and\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124mRecurrent Depressive Disorder. IT MUST BE IN PYTHON WITH THE PACKAGE pyDatalog.\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124mâ€¢ Mood Episode criterion: [Depressive, Manic, Mixed, and Hypomanic Episode criteria from ICD-11 CDDR].\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124mâ€¢ Relevant symptom names for Observed relation: [Symptom names]\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124mâ€¢ Relevant condition names for History relation: [Condition names]\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m---> 73\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT_SYSTEM\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     code \u001b[38;5;241m=\u001b[39m traitement_reponse(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     80\u001b[0m     check_expert(model, code)\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"llama-3.2\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "from src.inference import get_code_with_rag\n",
    "from src.config import PROMPT_SYSTEM_kanren ,PROMPT_SYSTEM_EXPERT_kanren \n",
    "get_code_with_rag(model = \"deepseek-r1\", model_expert = \"deepseek-r1\",w_model_expert = True,prompt_system=PROMPT_SYSTEM_kanren, prompt_system_expert=PROMPT_SYSTEM_EXPERT_kanren , lan_logic = 'karen') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', 'patient_1_diagnosis.py']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_prompt_patient_data_for_datalog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\notebooks\\..\\src\\inference.py:110\u001b[0m, in \u001b[0;36mget_prompt_patient_data_for_datalog\u001b[1;34m(patient_id)\u001b[0m\n\u001b[0;32m    107\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# ExÃ©cuter le script\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:524\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[1;32m--> 524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    525\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', 'patient_1_diagnosis.py']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from kanren import facts, var, run\n",
    "from diagnosis_kanren_rules import Observed, History, Diagnosis, PATIENT, rules\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_excel('/mnt/data/dataset.xlsx').fillna(\"Non renseignÃ©\")\n",
    "\n",
    "def diagnose_patient(patient_id):\n",
    "    # Injecter les rÃ¨gles de diagnostic\n",
    "    rules()\n",
    "\n",
    "    # Extraire les donnÃ©es du patient\n",
    "    patient_data = df[df['PatientID'] == patient_id]\n",
    "\n",
    "    # Injecter les facts Observed\n",
    "    for symptom, week in zip(patient_data[\"Observed_Symptom\"], patient_data[\"Observed_Week\"]):\n",
    "        facts(Observed, (patient_id, symptom, week))\n",
    "\n",
    "    # Injecter les facts History\n",
    "    for condition, count in zip(patient_data[\"History_Condition\"], patient_data[\"History_Count\"]):\n",
    "        if condition != \"Non renseignÃ©\":\n",
    "            facts(History, (patient_id, condition, count))\n",
    "\n",
    "    # ExÃ©cuter la requÃªte kanren\n",
    "    x = var()\n",
    "    result = run(0, x, Diagnosis(patient_id, x))\n",
    "    print(f\"Patient {patient_id} diagnosis: {result}\")\n",
    "\n",
    "# Exemple d'exÃ©cution pour tous les patients\n",
    "for pid in df['PatientID'].unique():\n",
    "    diagnose_patient(pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "  model=\"llama.3.2\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": PROMPT_SYSTEM},\n",
    "      {\"role\": \"user\", \"content\": \"Please translate Bipolar I and II criteria into python pyDatalog.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response['message'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT\n",
      "-------\n",
      "John   \n"
     ]
    }
   ],
   "source": [
    "from src import code_Datalog \n",
    "\n",
    "+ code_Datalog.Observed('John', 'Mania', 4.0)\n",
    "\n",
    "# Utiliser aussi les mÃªmes termes dÃ©clarÃ©s dans le mÃªme namespace\n",
    "print(code_Datalog.ManicEpisode(code_Datalog.PATIENT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(ManicEpisode(p))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(DepressionPeriod(p))\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(Diagnosis(p, \u001b[43mX\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import pyDatalog\n",
    "\n",
    "from pyDatalog import pyDatalog\n",
    "\n",
    "pyDatalog.clear()\n",
    "pyDatalog.create_terms('Patient, p, w, Count, Observed, ManicEpisode, DepressionPeriod, BipolarI, BipolarII, PsychoticFeatures, at_least_one_depression, at_least_two_episodes, Diagnosis')\n",
    "\n",
    "# ManicEpisode rules\n",
    "ManicEpisode(p) <= Observed(p, 'Mania', w) & (w >= 3.0)\n",
    "ManicEpisode(p) <= Observed(p, 'Hypomania', w) & (w >= 2.5)\n",
    "\n",
    "# Depression rules\n",
    "DepressionPeriod(p) <= Observed(p, 'MajorDepressiveDisorder', w) & ~(ManicEpisode(p))\n",
    "\n",
    "# PsychoticFeatures rule (commented out as in your original SoufflÃ©)\n",
    "# PsychoticFeatures(p) <= Observed(p, 'Psychosis', w)\n",
    "\n",
    "# Bipolar I diagnosis\n",
    "BipolarI(p) <= ManicEpisode(p) & at_least_one_depression(p) & at_least_two_episodes(p)\n",
    "\n",
    "# Bipolar II diagnosis (no psychosis)\n",
    "BipolarII(p) <= ManicEpisode(p) & ~(PsychoticFeatures(p)) & at_least_one_depression(p) & at_least_two_episodes(p)\n",
    "\n",
    "# Helper predicates\n",
    "at_least_one_depression(p) <= DepressionPeriod(p)\n",
    "# Here, pyDatalog does not have direct \"count()\" like SoufflÃ©, so you'll handle counts differently (see note below)\n",
    "# For now, we keep a placeholder\n",
    "at_least_two_episodes(p) <= (Count >= 2)  # Will need manual count handling with Python\n",
    "\n",
    "# Diagnosis outputs (print later)\n",
    "Diagnosis(p, 'Bipolar I') <= BipolarI(p)\n",
    "Diagnosis(p, 'Bipolar II') <= BipolarII(p)\n",
    "\n",
    "+ Observed('John', 'Mania', 4.0)\n",
    "+ Observed('John', 'MajorDepressiveDisorder', 3.0)\n",
    "+ Observed('John', 'MajorDepressiveDisorder', 3.0)\n",
    "\n",
    "print(ManicEpisode(p))\n",
    "print(DepressionPeriod(p))\n",
    "print(Diagnosis(p, X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM only approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import get_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"mistral\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistral\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\notebooks\\..\\src\\inference.py:7\u001b[0m, in \u001b[0;36mget_answer\u001b[1;34m(patient_id, model)\u001b[0m\n\u001b[0;32m      5\u001b[0m observed, history_conditions, mood_episodes \u001b[38;5;241m=\u001b[39m get_prompt_patient_data(patient_id) \n\u001b[0;32m      6\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor brevity, please output only the diagnosis for the following patients. Patients with no clear diagnosis should be indicated as such. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobserved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory_conditions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\FX506\\Desktop\\CS\\3A\\MVA\\LLM\\mva-llm-project\\myenv\\lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"mistral\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "get_answer(patient_id=10, model='mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "get_answer(patient_id=1, model='deepseek-r1:8b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 11):\n",
    "    print(f\"patient {i}\", get_answer(patient_id=i, model='deepseek-r1:8b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
